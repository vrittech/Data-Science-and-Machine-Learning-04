{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks (CNNs)\n",
    "\n",
    "Convolutional Neural Network or CNN, is a deep learning algorithm that is widely used for image and video recognition, as well as other tasks involving structured grid data. CNNs are designed to automatically and hierarchically learn complex features from raw input data without manual feature engineering. They have revolutionized the field of computer vision and achieved state-of-the-art performance on various image-related tasks.\n",
    "\n",
    "Here's a simplified explanation of how CNNs work:\n",
    "\n",
    "![CNN](assets/cnn.jpg)\n",
    "\n",
    "\n",
    "## Convolution Layer\n",
    "\n",
    "The input to a CNN is an image or a patch of an image. The first layer in a CNN is a convolutional layer that applies a set of learnable `filters` (also known as `kernels`) to the input image. Each filter performs a convolution operation by sliding across the image, computing dot products between the filter weights and the corresponding pixels in the input. This operation captures local patterns and features, such as edges, corners, and textures.\n",
    "\n",
    "### Kernels aka Filters\n",
    "\n",
    "Let's imagine you have a paragraph of text that you want to understand. When you read the paragraph, you start from the left and move to the right, extracting meaning from the words and sentences. You don't focus on individual letters, but rather interpret groups of letters as words. In a similar way, CNNs use kernels (also known as filters or feature detectors) to understand images.\n",
    "\n",
    "An image is made up of pixels, just like text is made up of letters. Each pixel in an image is associated with a value ranging from 0 to 255, determining its intensity (Or 3 values per pixel for colored images, one for each of red, green and blue channels). The intelligence of CNNs lies in these kernels because they capture the essential information from the image, allowing the network to make predictions or perform tasks like image classification, object detection, or image segmentation.\n",
    "\n",
    "![Kernels](assets/kernel.gif)\n",
    "\n",
    "**How do Kernels Work?**\n",
    "\n",
    "Imagine you have a grayscale image, like a black and white photo. Each pixel in the image has a specific value that represents its intensity. The kernel is a small matrix of values that we slide across the image.\n",
    "\n",
    "To extract a feature, we take the kernel and place it on top of a part of the image. We then multiply each pixel in the kernel with the corresponding pixel in the image. We add up all these multiplications to get a single number, which represents the presence or absence of a certain feature in that part of the image.\n",
    "\n",
    "This process is repeated as the kernel slides across the entire image, capturing different parts and extracting features from each. Each kernel focuses on detecting a specific pattern or feature. It gradually builds a hierarchical understanding, starting with low-level features like edges, corners and progressing to higher-level features like shapes, textures or objects.\n",
    "\n",
    "**How do I define the values of a Kernel?**\n",
    "\n",
    "The values in the kernel itself can be initialized randomly or set using a predefined function at the beginning. But as the network is trained with data, these values get adjusted to optimize the network's performance in extracting meaningful features.\n",
    "\n",
    "### Strides\n",
    "\n",
    "In real life each image is made up of thousands of tiny dots called pixels. Each pixel represents a color. When we want to process this picture using a Convolutional Neural Network (CNN), it can take a lot of time and computing power because there are so many pixels to consider.\n",
    "\n",
    "To make things faster and more efficient, we can reduce the number of features, which are essentially the different patterns or characteristics that the CNN tries to learn from the image. One way to do this is by changing the `stride` parameter. The stride parameter determines how many pixels the CNN's analyzing window, called a kernel or filter, moves at a time.\n",
    "\n",
    "In the previous example, we took a single step while moving the kernel i.e. the stride parameter has value `1` in this case. If you set two strides=2 then you take two step right in a row-wise and two steps down in a column-wise movement.\n",
    "\n",
    "### Padding\n",
    "\n",
    "In a Convolutional Neural Network (CNN), padding is used to add extra pixels around the borders of an image before applying convolution. Padding can help preserve spatial information, mitigate the loss of border pixels during convolution, and control the size of the output feature map. The added pixels create a buffer zone around the original image, allowing the kernel to traverse the borders and capture features from the entire image.\n",
    "\n",
    "There are two common types of padding:\n",
    "\n",
    "- **Valid (No Padding):** In this case, no padding is applied, and the kernel is only applied to positions where it fully overlaps with the input image. As a result, the output feature map will have smaller dimensions compared to the input image.\n",
    "\n",
    "- **Same (Zero Padding):** The \"same\" padding ensures that the output feature map has the same spatial dimensions (width and height) as the input image. To achieve this, the number of padding pixels added to each dimension is determined by the kernel size. For an odd-sized kernel, the same number of pixels is added to both sides, while for an even-sized kernel, the extra pixel is added to the right/bottom side.\n",
    "\n",
    "![Padding](assets/padding.png)\n",
    "\n",
    "Padding helps in several ways:\n",
    "\n",
    "- Retaining spatial dimensions: Zero padding ensures that the spatial dimensions of the input and output remain the same. This can be useful to maintain information at the borders of the image.\n",
    "\n",
    "- Centering the kernel: Padding allows the kernel to be centered at the borders of the image, enabling it to process pixels in the border regions.\n",
    "\n",
    "- Controlling downsampling: Padding can prevent significant downsampling of the image by reducing the stride effect. It helps in preserving more spatial information throughout the convolutional layers.\n",
    "\n",
    "- Mitigating information loss: Padding helps in reducing the loss of important border pixels during convolution, which can be crucial in tasks like object detection or segmentation.\n",
    "\n",
    "The output size after applying kernels, strides and padding is given by:\n",
    "\n",
    "$Feature\\ Map\\ Size = \\frac{Input\\ Image\\ size - Kernel\\ size + 2 * Padding}{Stride} + 1$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "\n",
    "After the convolution operation, an element-wise activation function (typically ReLU - Rectified Linear Unit) is applied to introduce non-linearity. This helps the network learn more complex representations.\n",
    "\n",
    "## Pooling Layer\n",
    "\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"./assets/pooling.mp4\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "\n",
    "The video above is a very good demonstration of an insight that reducing the size of an image does not always result in information loss, as evidenced by the ability to identify a dog in a smaller version of the picture. Even a fraction (1/4th) of the original image is sufficient to deduce its content, obviating the need for processing the entire image in CNNs.\n",
    "\n",
    "Pooling plays a crucial role in this scenario, facilitating the reduction of the feature map's size.\n",
    "\n",
    "The primary purpose of pooling is to reduce the spatial dimensions of the feature map, maintaining relevant information while enhancing computational efficiency. Pooling achieves size reduction of the image (feature map) by employing a filter that extracts a single value from a block of features. The most common type of pooling is max pooling, which divides the input into small regions and outputs the maximum value within each region. This downsampling operation reduces the computational complexity and makes the network more robust to spatial translations and distortions.\n",
    "\n",
    "![Pooling Workflow](assets/pooling.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening and Fully Connected Layers\n",
    "\n",
    "After several convolutional and pooling layers, the output is flattened and fed into one or more fully connected layers. These layers are similar to traditional artificial neural networks, where each neuron is connected to every neuron in the previous layer. The fully connected layers capture high-level features and patterns by learning complex combinations of the low-level features obtained from earlier layers.\n",
    "\n",
    "![Flattening and Fully Connected Layers](./assets/flatten.jpg)\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "The final fully connected layer is usually followed by an output layer with a suitable activation function depending on the task at hand. For instance, for image classification problems, a softmax activation is commonly used to produce a probability distribution over different classes.\n",
    "\n",
    "## Training and Backpropagation\n",
    "\n",
    "The CNN is trained using a large labeled dataset. During training, the network adjusts the weights of the filters and the fully connected layers to minimize the difference between the predicted outputs and the ground truth labels. This process is typically performed using an optimization algorithm called backpropagation, which computes the gradients of the loss function with respect to the network's parameters and updates them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │           \u001b[38;5;34m448\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m36,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,458</span> (142.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,458\u001b[0m (142.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,458</span> (142.41 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,458\u001b[0m (142.41 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(32, 32, 3)),  # Convolutional layer with 16 filters and 3x3 kernel size\n",
    "    layers.MaxPooling2D((2, 2)),  # Max pooling layer with 2x2 pool size\n",
    "    layers.Flatten(),  # Flatten the feature maps\n",
    "    layers.Dense(10, activation='softmax')  # Fully connected layer with 10 output units (for classification)\n",
    "])\n",
    "\n",
    "# Print the model summary:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (32, 32, 3)\n",
      "Label: tf.Tensor(8, shape=(), dtype=int32)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 12:14:10.227662: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1332 - loss: 2.9993  \n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1268 - loss: 2.8302\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2910 - loss: 2.0253\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5335 - loss: 1.6111\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5514 - loss: 1.5017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x76168832c130>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a custom Tensor dataset\n",
    "data = tf.random.normal((100, 32, 32, 3))  # Example dataset with 100 images of shape 32x32 and 3 channels\n",
    "labels = tf.random.uniform((100,), maxval=10, dtype=tf.int32)  # Example labels for the dataset\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "# Print a sample from the dataset\n",
    "for image, label in dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Label:\", label)\n",
    "\n",
    "# Compile and train the model (you should use your own dataset and training setup)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(dataset.batch(32), epochs=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a simple CNN model using the Sequential API from TensorFlow's Keras module. The model consists of a convolutional layer with a 3x3 kernel, followed by a max pooling layer, a flattening layer, and a fully connected layer for classification. The model's summary provides information about the layers, including the kernel representation.\n",
    "\n",
    "We then create a custom Tensor dataset using TensorFlow's `tf.data.Dataset.from_tensor_slices` method. You can replace this part with your own dataset loading and preprocessing code. In this example, we generate random tensors for the dataset images and labels.\n",
    "\n",
    "Finally, we compile the model using an optimizer, loss function, and metrics, and train it using the dataset. Note that in this example, we are training for only 5 epochs with a batch size of 32. You should adjust these parameters and use your own dataset and training setup according to your specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x761688346d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1350 - loss: 2.7274 \n",
      "Test Loss: 2.7336249351501465 \tTest Accuracy: 0.14000000059604645\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x761688346ef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted Labels: tf.Tensor([5 7 5 5 5 5 2 5 2 2], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test_data = tf.random.normal((50, 32, 32, 3))  # Example test dataset with 50 images\n",
    "test_labels = tf.random.uniform((50,), maxval=10, dtype=tf.int32)  # Example test labels\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "print(\"Test Loss:\", test_loss, \"\\tTest Accuracy:\", test_accuracy)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = tf.random.normal((10, 32, 32, 3))  # Example new data with 10 images\n",
    "\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# The predictions will be a probability distribution over the classes\n",
    "# Extract the predicted class labels using argmax\n",
    "predicted_labels = tf.argmax(predictions, axis=-1)\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, assuming you have a separate test dataset (with test data and corresponding labels), you can evaluate the model's performance by calling the `evaluate` method on the test data and labels. It will return the test loss and accuracy.\n",
    "\n",
    "To make predictions on new data using the trained model, you can call the `predict` method, passing in the new data. The output will be a probability distribution over the classes. You can extract the predicted class labels using `argmax` along the appropriate axis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
