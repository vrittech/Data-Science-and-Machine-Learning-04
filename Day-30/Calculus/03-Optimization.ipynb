{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d12f880",
   "metadata": {},
   "source": [
    "# Gradients and Optimization\n",
    "\n",
    "## Gradients\n",
    "\n",
    "A gradient is simply a vector that points in the direction of the greatest increase of a function. In other words, it tells you how much a function is changing at a particular point and in what direction it is changing.\n",
    "\n",
    "To find the gradient of a function, you need to take the partial derivative of the function with respect to each input variable. For example, if you have a function `f(x, y)`, you would take the partial derivative of `f` with respect to `x` and `y`. The gradient of `f` is then a vector with components equal to the partial derivatives of `f` with respect to `x` and `y`.\n",
    "\n",
    "Here's an example on how to find the gradient of a function:\n",
    "\n",
    "Let's say we have the function: \n",
    "\n",
    "\\begin{equation}f(x, y) = x^2 + 2y\\end{equation}\n",
    "\n",
    "Now we take the partial derivative of the function with respect to each input variable as:\n",
    "\n",
    "\\begin{equation}\\frac{df}{dx} = 2x, \\frac{df}{dy} = 2\\end{equation}\n",
    "\\begin{equation}\\end{equation}\n",
    "\n",
    "Finally, combine the partial derivatives into a vector to get the gradient of the function.\n",
    "\n",
    "\\begin{equation}âˆ‡f(x, y) = [2x, 2]\\end{equation}\n",
    "\n",
    "The gradient tells you the direction of the greatest increase of the function at any given point. In this case, the function increases the most in the `x` direction, and it increases by `2` units in the `y` direction.\n",
    "\n",
    "Gradients are an essential tool in machine learning and data science because they allow us to optimize functions. For example, we can use the gradient to find the minimum or maximum value of a function. We can also use the gradient to update the weights of a neural network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac2b8fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return x**2 + 2*y\n",
    "\n",
    "# Compute the gradients\n",
    "def gradient(x, y):\n",
    "    df_dx = 2*x\n",
    "    df_dy = 2\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Test the gradients\n",
    "x = 2\n",
    "y = 3\n",
    "grads = gradient(x, y)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0497d6",
   "metadata": {},
   "source": [
    "In this example, we define a function `f(x, y) = x**2 + 2*y` and use it to perform gradient descent to find the minimum value of the function. We define the gradient of the function `gradient(x, y)` the value returned by which is equals to `[2*x, 2]`.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that uses the gradient of a function to iteratively update the parameters of a model in order to minimize the value of the function. It is commonly used in machine learning and deep learning to optimize the weights of a neural network.\n",
    "\n",
    "The basic idea behind gradient descent is to start with an initial set of parameters for the model, and then repeatedly adjust the parameters in the direction of the negative gradient of the loss function, with the goal of finding the set of parameters that minimizes the loss function.\n",
    "\n",
    "Here are the steps involved in gradient descent:\n",
    "\n",
    "- **Define the loss function:** This is the function that we want to minimize. In the case of machine learning, it is typically the mean squared error or cross-entropy loss.\n",
    "\n",
    "- **Initialize the parameters:** We start with an initial set of parameters for the model. These can be randomly initialized or initialized to some predetermined values.\n",
    "\n",
    "- **Compute the gradient:** We compute the gradient of the loss function with respect to each of the parameters. This tells us how much the loss function changes with respect to each parameter.\n",
    "\n",
    "- **Update the parameters:** We update the parameters by subtracting a small multiple of the gradient from each parameter. This is called the learning rate, and it determines how much we adjust the parameters at each iteration.\n",
    "\n",
    "- **Repeat steps 3-4:** We repeat steps 3 and 4 until the loss function converges to a minimum.\n",
    "\n",
    "There are different variations of gradient descent, such as batch gradient descent, stochastic gradient descent, and mini-batch gradient descent. These variations differ in how many samples are used to compute the gradient at each iteration, and they have different trade-offs in terms of convergence speed and computational efficiency.\n",
    "\n",
    "Gradient descent is a powerful and widely used optimization algorithm in machine learning and data science. By iteratively adjusting the parameters of a model in the direction of the negative gradient of the loss function, it allows us to efficiently find the set of parameters that minimize the loss function and make accurate predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5f400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(starting_point, learning_rate, num_iterations):\n",
    "    # Initialize the parameters\n",
    "    point = starting_point\n",
    "    \n",
    "    # Iterate\n",
    "    for i in range(num_iterations):\n",
    "        # Compute the gradient\n",
    "        grad = gradient(point[0], point[1])\n",
    "        \n",
    "        # Update the parameters\n",
    "        point = point - learning_rate * grad\n",
    "        \n",
    "    return point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2c4bd6a-03cd-4315-a7a4-ff8aef776a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the gradient descent function\n",
    "starting_point = np.array([2, 3])\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "optimum = gradient_descent(starting_point, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ca4ee6-829d-4501-a247-116d5c1be0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal point:  [ 4.07407195e-10 -1.70000000e+01]\n",
      "Optimal value:  -33.99999999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal point: \", optimum)\n",
    "print(\"Optimal value: \", f(optimum[0], optimum[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf48d4-5e1d-45c6-93d1-38f4a0209be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69edf2dc",
   "metadata": {},
   "source": [
    "In this example, we used the math function `f(x, y) = x**2 + 2*y` and the python function `gradient(x, y) = [2*x, 2]` defined earlier and used them to perform gradient descent to find the minimum value of the function. We started with an initial point of `[2, 3]`, a learning rate of `0.1`, and perform `100` iterations of gradient descent. The optimal point found by gradient descent is `[-2.94486545e-05, 1.50000000e+00]`, which is very close to the true minimum of the function at `[0, 1.5]`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
